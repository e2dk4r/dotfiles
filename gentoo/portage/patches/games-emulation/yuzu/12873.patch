From 42f7f6b6fdd9f40e9709b790f0a6df0007307333 Mon Sep 17 00:00:00 2001
From: Fernando Sahmkow <fsahmkow27@gmail.com>
Date: Fri, 5 Jan 2024 01:51:42 +0100
Subject: [PATCH 1/2] GPU: Implement channel scheduling.

---
 src/video_core/control/scheduler.cpp | 94 ++++++++++++++++++++++++++--
 src/video_core/control/scheduler.h   | 25 ++++++++
 src/video_core/engines/puller.cpp    | 12 +++-
 src/video_core/gpu.cpp               |  8 +++
 src/video_core/gpu.h                 |  9 ++-
 src/video_core/gpu_thread.cpp        | 14 +++--
 src/video_core/gpu_thread.h          |  9 +--
 7 files changed, 149 insertions(+), 22 deletions(-)

diff --git a/src/video_core/control/scheduler.cpp b/src/video_core/control/scheduler.cpp
index f7cbe204ee8c..3165f9bf368b 100644
--- a/src/video_core/control/scheduler.cpp
+++ b/src/video_core/control/scheduler.cpp
@@ -13,20 +13,102 @@ Scheduler::Scheduler(GPU& gpu_) : gpu{gpu_} {}
 
 Scheduler::~Scheduler() = default;
 
+void Scheduler::Init() {
+    master_control = Common::Fiber::ThreadToFiber();
+}
+
+void Scheduler::Resume() {
+    bool nothing_pending;
+    do {
+        nothing_pending = true;
+        current_fifo = nullptr;
+        {
+            std::unique_lock lk(scheduling_guard);
+            size_t num_iters = gpfifos.size();
+            for (size_t i = 0; i < num_iters; i++) {
+                size_t current_id = (current_fifo_rotation_id + i) % gpfifos.size();
+                auto& fifo = gpfifos[current_id];
+                if (!fifo.is_active) {
+                    continue;
+                }
+                std::scoped_lock lk2(fifo.guard);
+                if (!fifo.pending_work.empty() || fifo.working.load(std::memory_order_acquire)) {
+                    current_fifo = &fifo;
+                    current_fifo_rotation_id = current_id;
+                    nothing_pending = false;
+                    break;
+                }
+            }
+        }
+        if (current_fifo) {
+            Common::Fiber::YieldTo(master_control, *current_fifo->context);
+            current_fifo = nullptr;
+        }
+    } while (!nothing_pending);
+}
+
+void Scheduler::Yield() {
+    ASSERT(current_fifo != nullptr);
+    Common::Fiber::YieldTo(current_fifo->context, *master_control);
+    gpu.BindChannel(current_fifo->bind_id);
+}
+
 void Scheduler::Push(s32 channel, CommandList&& entries) {
     std::unique_lock lk(scheduling_guard);
-    auto it = channels.find(channel);
-    ASSERT(it != channels.end());
-    auto channel_state = it->second;
-    gpu.BindChannel(channel_state->bind_id);
-    channel_state->dma_pusher->Push(std::move(entries));
-    channel_state->dma_pusher->DispatchCalls();
+    auto it = channel_gpfifo_ids.find(channel);
+    ASSERT(it != channel_gpfifo_ids.end());
+    auto gpfifo_id = it->second;
+    auto& fifo = gpfifos[gpfifo_id];
+    {
+        std::scoped_lock lk2(fifo.guard);
+        fifo.pending_work.emplace_back(std::move(entries));
+    }
+}
+
+void Scheduler::ChannelLoop(size_t gpfifo_id, s32 channel_id) {
+    gpu.BindChannel(channel_id);
+    auto& fifo = gpfifos[gpfifo_id];
+    while (true) {
+        auto* channel_state = channels[channel_id].get();
+        fifo.guard.lock();
+        while (!fifo.pending_work.empty()) {
+            {
+
+                fifo.working.store(true, std::memory_order_release);
+                CommandList&& entries = std::move(fifo.pending_work.front());
+                channel_state->dma_pusher->Push(std::move(entries));
+                fifo.pending_work.pop_front();
+            }
+            fifo.guard.unlock();
+            channel_state->dma_pusher->DispatchCalls();
+            fifo.guard.lock();
+        }
+        fifo.working.store(false, std::memory_order_relaxed);
+        fifo.guard.unlock();
+        Common::Fiber::YieldTo(fifo.context, *master_control);
+        gpu.BindChannel(channel_id);
+    }
 }
 
 void Scheduler::DeclareChannel(std::shared_ptr<ChannelState> new_channel) {
     s32 channel = new_channel->bind_id;
     std::unique_lock lk(scheduling_guard);
     channels.emplace(channel, new_channel);
+    size_t new_fifo_id;
+    if (!free_fifos.empty()) {
+        new_fifo_id = free_fifos.front();
+        free_fifos.pop_front();
+    } else {
+        new_fifo_id = gpfifos.size();
+        gpfifos.emplace_back();
+    }
+    auto& new_fifo = gpfifos[new_fifo_id];
+    channel_gpfifo_ids[channel] = new_fifo_id;
+    new_fifo.is_active = true;
+    new_fifo.bind_id = channel;
+    new_fifo.pending_work.clear();
+    std::function<void()> callback = std::bind(&Scheduler::ChannelLoop, this, new_fifo_id, channel);
+    new_fifo.context = std::make_shared<Common::Fiber>(std::move(callback));
 }
 
 } // namespace Tegra::Control
diff --git a/src/video_core/control/scheduler.h b/src/video_core/control/scheduler.h
index 44addf61c4bc..c6f374f66a29 100644
--- a/src/video_core/control/scheduler.h
+++ b/src/video_core/control/scheduler.h
@@ -3,10 +3,13 @@
 
 #pragma once
 
+#include <atomic>
+#include <deque>
 #include <memory>
 #include <mutex>
 #include <unordered_map>
 
+#include "common/fiber.h"
 #include "video_core/dma_pusher.h"
 
 namespace Tegra {
@@ -22,14 +25,36 @@ class Scheduler {
     explicit Scheduler(GPU& gpu_);
     ~Scheduler();
 
+    void Init();
+
+    void Resume();
+
+    void Yield();
+
     void Push(s32 channel, CommandList&& entries);
 
     void DeclareChannel(std::shared_ptr<ChannelState> new_channel);
 
 private:
+    void ChannelLoop(size_t gpfifo_id, s32 channel_id);
+
     std::unordered_map<s32, std::shared_ptr<ChannelState>> channels;
+    std::unordered_map<s32, size_t> channel_gpfifo_ids;
     std::mutex scheduling_guard;
+    std::shared_ptr<Common::Fiber> master_control;
+    struct GPFifoContext {
+        bool is_active;
+        std::shared_ptr<Common::Fiber> context;
+        std::deque<CommandList> pending_work;
+        std::atomic<bool> working{};
+        std::mutex guard;
+        s32 bind_id;
+    };
+    std::deque<GPFifoContext> gpfifos;
+    std::deque<size_t> free_fifos;
     GPU& gpu;
+    size_t current_fifo_rotation_id{};
+    GPFifoContext* current_fifo{};
 };
 
 } // namespace Control
diff --git a/src/video_core/engines/puller.cpp b/src/video_core/engines/puller.cpp
index 8dd34c04ab4f..f51dbe47e7ae 100644
--- a/src/video_core/engines/puller.cpp
+++ b/src/video_core/engines/puller.cpp
@@ -6,6 +6,7 @@
 #include "common/settings.h"
 #include "core/core.h"
 #include "video_core/control/channel_state.h"
+#include "video_core/control/scheduler.h"
 #include "video_core/dma_pusher.h"
 #include "video_core/engines/fermi_2d.h"
 #include "video_core/engines/kepler_compute.h"
@@ -14,6 +15,8 @@
 #include "video_core/engines/maxwell_dma.h"
 #include "video_core/engines/puller.h"
 #include "video_core/gpu.h"
+#include "video_core/host1x/host1x.h"
+#include "video_core/host1x/syncpoint_manager.h"
 #include "video_core/memory_manager.h"
 #include "video_core/rasterizer_interface.h"
 
@@ -60,11 +63,14 @@ void Puller::ProcessBindMethod(const MethodCall& method_call) {
 }
 
 void Puller::ProcessFenceActionMethod() {
+    auto& syncpoint_manager = gpu.Host1x().GetSyncpointManager();
     switch (regs.fence_action.op) {
     case Puller::FenceOperation::Acquire:
-        // UNIMPLEMENTED_MSG("Channel Scheduling pending.");
-        // WaitFence(regs.fence_action.syncpoint_id, regs.fence_value);
-        rasterizer->ReleaseFences();
+        while (regs.fence_value >
+               syncpoint_manager.GetGuestSyncpointValue(regs.fence_action.syncpoint_id)) {
+            rasterizer->ReleaseFences();
+            gpu.Scheduler().Yield();
+        }
         break;
     case Puller::FenceOperation::Increment:
         rasterizer->SignalSyncPoint(regs.fence_action.syncpoint_id);
diff --git a/src/video_core/gpu.cpp b/src/video_core/gpu.cpp
index f4a5d831cdeb..e6066f0e58ef 100644
--- a/src/video_core/gpu.cpp
+++ b/src/video_core/gpu.cpp
@@ -401,6 +401,14 @@ std::shared_ptr<Control::ChannelState> GPU::AllocateChannel() {
     return impl->AllocateChannel();
 }
 
+Tegra::Control::Scheduler& GPU::Scheduler() {
+    return *impl->scheduler;
+}
+
+const Tegra::Control::Scheduler& GPU::Scheduler() const {
+    return *impl->scheduler;
+}
+
 void GPU::InitChannel(Control::ChannelState& to_init) {
     impl->InitChannel(to_init);
 }
diff --git a/src/video_core/gpu.h b/src/video_core/gpu.h
index c4602ca372fb..dd24d2a6e91c 100644
--- a/src/video_core/gpu.h
+++ b/src/video_core/gpu.h
@@ -124,7 +124,8 @@ class KeplerCompute;
 
 namespace Control {
 struct ChannelState;
-}
+class Scheduler;
+} // namespace Control
 
 namespace Host1x {
 class Host1x;
@@ -204,6 +205,12 @@ class GPU final {
     /// Returns a const reference to the shader notifier.
     [[nodiscard]] const VideoCore::ShaderNotify& ShaderNotify() const;
 
+    /// Returns GPU Channel Scheduler.
+    [[nodiscard]] Tegra::Control::Scheduler& Scheduler();
+
+    /// Returns GPU Channel Scheduler.
+    [[nodiscard]] const Tegra::Control::Scheduler& Scheduler() const;
+
     [[nodiscard]] u64 GetTicks() const;
 
     [[nodiscard]] bool IsAsync() const;
diff --git a/src/video_core/gpu_thread.cpp b/src/video_core/gpu_thread.cpp
index 58d8110b8646..498f99d391cf 100644
--- a/src/video_core/gpu_thread.cpp
+++ b/src/video_core/gpu_thread.cpp
@@ -33,13 +33,15 @@ static void RunThread(std::stop_token stop_token, Core::System& system,
 
     CommandDataContainer next;
 
+    scheduler.Init();
+
     while (!stop_token.stop_requested()) {
         state.queue.PopWait(next, stop_token);
         if (stop_token.stop_requested()) {
             break;
         }
-        if (auto* submit_list = std::get_if<SubmitListCommand>(&next.data)) {
-            scheduler.Push(submit_list->channel, std::move(submit_list->entries));
+        if (std::holds_alternative<SubmitListCommand>(next.data)) {
+            scheduler.Resume();
         } else if (std::holds_alternative<GPUTickCommand>(next.data)) {
             system.GPU().TickWork();
         } else if (const auto* flush = std::get_if<FlushRegionCommand>(&next.data)) {
@@ -66,14 +68,16 @@ ThreadManager::~ThreadManager() = default;
 
 void ThreadManager::StartThread(VideoCore::RendererBase& renderer,
                                 Core::Frontend::GraphicsContext& context,
-                                Tegra::Control::Scheduler& scheduler) {
+                                Tegra::Control::Scheduler& scheduler_) {
     rasterizer = renderer.ReadRasterizer();
+    scheduler = &scheduler_;
     thread = std::jthread(RunThread, std::ref(system), std::ref(renderer), std::ref(context),
-                          std::ref(scheduler), std::ref(state));
+                          std::ref(scheduler_), std::ref(state));
 }
 
 void ThreadManager::SubmitList(s32 channel, Tegra::CommandList&& entries) {
-    PushCommand(SubmitListCommand(channel, std::move(entries)));
+    scheduler->Push(channel, std::move(entries));
+    PushCommand(SubmitListCommand());
 }
 
 void ThreadManager::FlushRegion(DAddr addr, u64 size) {
diff --git a/src/video_core/gpu_thread.h b/src/video_core/gpu_thread.h
index dc0fce9f8244..52798fde64d7 100644
--- a/src/video_core/gpu_thread.h
+++ b/src/video_core/gpu_thread.h
@@ -36,13 +36,7 @@ class RendererBase;
 namespace VideoCommon::GPUThread {
 
 /// Command to signal to the GPU thread that a command list is ready for processing
-struct SubmitListCommand final {
-    explicit SubmitListCommand(s32 channel_, Tegra::CommandList&& entries_)
-        : channel{channel_}, entries{std::move(entries_)} {}
-
-    s32 channel;
-    Tegra::CommandList entries;
-};
+struct SubmitListCommand final {};
 
 /// Command to signal to the GPU thread to flush a region
 struct FlushRegionCommand final {
@@ -124,6 +118,7 @@ class ThreadManager final {
 private:
     /// Pushes a command to be executed by the GPU thread
     u64 PushCommand(CommandData&& command_data, bool block = false);
+    Tegra::Control::Scheduler* scheduler;
 
     Core::System& system;
     const bool is_async;

From 7a4ea8991f811d118e92e569c7688a998356f09b Mon Sep 17 00:00:00 2001
From: Fernando Sahmkow <fsahmkow27@gmail.com>
Date: Sat, 10 Feb 2024 18:56:37 +0100
Subject: [PATCH 2/2] GPU/Scheduling: Implement priority scheduling.

---
 .../hle/service/nvdrv/devices/nvhost_gpu.cpp  |   7 +
 src/video_core/control/channel_state.h        |   6 +
 src/video_core/control/scheduler.cpp          | 251 +++++++++++++-----
 src/video_core/control/scheduler.h            |  32 +--
 4 files changed, 215 insertions(+), 81 deletions(-)

diff --git a/src/core/hle/service/nvdrv/devices/nvhost_gpu.cpp b/src/core/hle/service/nvdrv/devices/nvhost_gpu.cpp
index bf12d69a5d68..9540008c52d0 100644
--- a/src/core/hle/service/nvdrv/devices/nvhost_gpu.cpp
+++ b/src/core/hle/service/nvdrv/devices/nvhost_gpu.cpp
@@ -13,6 +13,7 @@
 #include "core/hle/service/nvdrv/nvdrv.h"
 #include "core/memory.h"
 #include "video_core/control/channel_state.h"
+#include "video_core/control/scheduler.h"
 #include "video_core/engines/puller.h"
 #include "video_core/gpu.h"
 #include "video_core/host1x/host1x.h"
@@ -33,6 +34,7 @@ nvhost_gpu::nvhost_gpu(Core::System& system_, EventInterface& events_interface_,
       syncpoint_manager{core_.GetSyncpointManager()}, nvmap{core.GetNvMapFile()},
       channel_state{system.GPU().AllocateChannel()} {
     channel_syncpoint = syncpoint_manager.AllocateSyncpoint(false);
+    channel_state->syncpoint_id = channel_syncpoint;
     sm_exception_breakpoint_int_report_event =
         events_interface.CreateEvent("GpuChannelSMExceptionBreakpointInt");
     sm_exception_breakpoint_pause_report_event =
@@ -157,6 +159,9 @@ NvResult nvhost_gpu::SetErrorNotifier(IoctlSetErrorNotifier& params) {
 
 NvResult nvhost_gpu::SetChannelPriority(IoctlChannelSetPriority& params) {
     channel_priority = params.priority;
+    if (channel_state->initialized) {
+        system.GPU().Scheduler().ChangePriority(channel_state->bind_id, channel_priority);
+    }
     LOG_DEBUG(Service_NVDRV, "(STUBBED) called, priority={:X}", channel_priority);
     return NvResult::Success;
 }
@@ -314,6 +319,7 @@ NvResult nvhost_gpu::GetWaitbase(IoctlGetWaitbase& params) {
 NvResult nvhost_gpu::ChannelSetTimeout(IoctlChannelSetTimeout& params) {
     LOG_INFO(Service_NVDRV, "called, timeout=0x{:X}", params.timeout);
 
+    channel_state->timeout = params.timeout;
     return NvResult::Success;
 }
 
@@ -321,6 +327,7 @@ NvResult nvhost_gpu::ChannelSetTimeslice(IoctlSetTimeslice& params) {
     LOG_INFO(Service_NVDRV, "called, timeslice=0x{:X}", params.timeslice);
 
     channel_timeslice = params.timeslice;
+    channel_state->timeslice = params.timeslice;
 
     return NvResult::Success;
 }
diff --git a/src/video_core/control/channel_state.h b/src/video_core/control/channel_state.h
index 3a7b9872c193..97b225391ea6 100644
--- a/src/video_core/control/channel_state.h
+++ b/src/video_core/control/channel_state.h
@@ -45,6 +45,12 @@ struct ChannelState {
     void BindRasterizer(VideoCore::RasterizerInterface* rasterizer);
 
     s32 bind_id = -1;
+    /// Scheduling info
+    u32 syncpoint_id = 0xFFFF;
+    u32 priority = 0;
+    u32 timeslice = 0;
+    u32 timeout = 0;
+
     /// 3D engine
     std::unique_ptr<Engines::Maxwell3D> maxwell_3d;
     /// 2D engine
diff --git a/src/video_core/control/scheduler.cpp b/src/video_core/control/scheduler.cpp
index 3165f9bf368b..9ee2a6fb25ce 100644
--- a/src/video_core/control/scheduler.cpp
+++ b/src/video_core/control/scheduler.cpp
@@ -1,112 +1,243 @@
 // SPDX-FileCopyrightText: 2021 yuzu Emulator Project
 // SPDX-License-Identifier: GPL-3.0-or-later
 
+#include <atomic>
+#include <deque>
+#include <map>
 #include <memory>
+#include <mutex>
+#include <unordered_map>
+#include <utility>
 
 #include "common/assert.h"
-#include "video_core/control/channel_state.h"
+#include "common/fiber.h"
 #include "video_core/control/scheduler.h"
+#include "video_core/dma_pusher.h"
 #include "video_core/gpu.h"
 
 namespace Tegra::Control {
-Scheduler::Scheduler(GPU& gpu_) : gpu{gpu_} {}
+
+struct GPFifoContext {
+    bool is_active;
+    bool is_running;
+    std::shared_ptr<Common::Fiber> context;
+    std::deque<CommandList> pending_work;
+    std::mutex guard;
+    s32 bind_id;
+    std::shared_ptr<ChannelState> info;
+    size_t yield_count;
+    size_t scheduled_count;
+};
+
+struct Scheduler::SchedulerImpl {
+    // Fifos
+    std::map<u32, std::list<size_t>, std::greater<u32>> schedule_priority_queue;
+    std::unordered_map<s32, size_t> channel_gpfifo_ids;
+    std::deque<GPFifoContext> gpfifos;
+    std::deque<size_t> free_fifos;
+
+    // Scheduling
+    std::mutex scheduling_guard;
+    std::shared_ptr<Common::Fiber> master_control;
+    bool must_reschedule{};
+    GPFifoContext* current_fifo{};
+};
+
+Scheduler::Scheduler(GPU& gpu_) : gpu{gpu_} {
+    impl = std::make_unique<SchedulerImpl>();
+}
 
 Scheduler::~Scheduler() = default;
 
 void Scheduler::Init() {
-    master_control = Common::Fiber::ThreadToFiber();
+    impl->master_control = Common::Fiber::ThreadToFiber();
 }
 
 void Scheduler::Resume() {
-    bool nothing_pending;
-    do {
-        nothing_pending = true;
-        current_fifo = nullptr;
-        {
-            std::unique_lock lk(scheduling_guard);
-            size_t num_iters = gpfifos.size();
-            for (size_t i = 0; i < num_iters; i++) {
-                size_t current_id = (current_fifo_rotation_id + i) % gpfifos.size();
-                auto& fifo = gpfifos[current_id];
-                if (!fifo.is_active) {
-                    continue;
-                }
-                std::scoped_lock lk2(fifo.guard);
-                if (!fifo.pending_work.empty() || fifo.working.load(std::memory_order_acquire)) {
-                    current_fifo = &fifo;
-                    current_fifo_rotation_id = current_id;
-                    nothing_pending = false;
-                    break;
-                }
-            }
+    while (UpdateHighestPriorityChannel()) {
+        impl->current_fifo->scheduled_count++;
+        Common::Fiber::YieldTo(impl->master_control, *impl->current_fifo->context);
+    }
+}
+
+bool Scheduler::UpdateHighestPriorityChannel() {
+    std::scoped_lock lk(impl->scheduling_guard);
+
+    // Clear needs to schedule state.
+    impl->must_reschedule = false;
+
+    // By default, we don't have a channel to schedule.
+    impl->current_fifo = nullptr;
+
+    // Check each level to see if we can schedule.
+    for (auto& level : impl->schedule_priority_queue) {
+        if (ScheduleLevel(level.second)) {
+            return true;
+        }
+    }
+
+    // Nothing to schedule.
+    return false;
+}
+
+bool Scheduler::ScheduleLevel(std::list<size_t>& queue) {
+    bool found_anything = false;
+    size_t min_schedule_count = std::numeric_limits<size_t>::max();
+    for (auto id : queue) {
+        auto& fifo = impl->gpfifos[id];
+        std::scoped_lock lk(fifo.guard);
+
+        // With no pending work and nothing running, this channel can't be scheduled.
+        if (fifo.pending_work.empty() && !fifo.is_running) {
+            continue;
         }
-        if (current_fifo) {
-            Common::Fiber::YieldTo(master_control, *current_fifo->context);
-            current_fifo = nullptr;
+        // Prioritize channels at current priority which have been run the least.
+        if (fifo.scheduled_count > min_schedule_count) {
+            continue;
         }
-    } while (!nothing_pending);
+
+        // Try not to select the same channel we just yielded from.
+        if (fifo.scheduled_count < fifo.yield_count) {
+            fifo.scheduled_count++;
+            continue;
+        }
+
+        // Update best selection.
+        min_schedule_count = fifo.scheduled_count;
+        impl->current_fifo = &fifo;
+        found_anything = true;
+    }
+    return found_anything;
+}
+
+void Scheduler::ChangePriority(s32 channel_id, u32 new_priority) {
+    std::scoped_lock lk(impl->scheduling_guard);
+    // Ensure we are tracking this channel.
+    auto fifo_it = impl->channel_gpfifo_ids.find(channel_id);
+    if (fifo_it == impl->channel_gpfifo_ids.end()) {
+        return;
+    }
+
+    // Get the fifo and update its priority.
+    const size_t fifo_id = fifo_it->second;
+    auto& fifo = impl->gpfifos[fifo_id];
+    const auto old_priority = std::exchange(fifo.info->priority, new_priority);
+
+    // Create the new level if needed.
+    impl->schedule_priority_queue.try_emplace(new_priority);
+
+    // Remove the old level and add to the new level.
+    impl->schedule_priority_queue[new_priority].push_back(fifo_id);
+    impl->schedule_priority_queue[old_priority].remove_if(
+        [fifo_id](size_t id) { return id == fifo_id; });
 }
 
 void Scheduler::Yield() {
-    ASSERT(current_fifo != nullptr);
-    Common::Fiber::YieldTo(current_fifo->context, *master_control);
-    gpu.BindChannel(current_fifo->bind_id);
+    ASSERT(impl->current_fifo != nullptr);
+
+    // Set yield count higher
+    impl->current_fifo->yield_count = impl->current_fifo->scheduled_count + 1;
+    Common::Fiber::YieldTo(impl->current_fifo->context, *impl->master_control);
+    gpu.BindChannel(impl->current_fifo->bind_id);
+}
+
+void Scheduler::CheckStatus() {
+    {
+        std::unique_lock lk(impl->scheduling_guard);
+        // If no reschedule is needed, don't transfer control
+        if (!impl->must_reschedule) {
+            return;
+        }
+    }
+    // Transfer control to the scheduler
+    Common::Fiber::YieldTo(impl->current_fifo->context, *impl->master_control);
+    gpu.BindChannel(impl->current_fifo->bind_id);
 }
 
 void Scheduler::Push(s32 channel, CommandList&& entries) {
-    std::unique_lock lk(scheduling_guard);
-    auto it = channel_gpfifo_ids.find(channel);
-    ASSERT(it != channel_gpfifo_ids.end());
+    std::scoped_lock lk(impl->scheduling_guard);
+    // Get and ensure we have this channel.
+    auto it = impl->channel_gpfifo_ids.find(channel);
+    ASSERT(it != impl->channel_gpfifo_ids.end());
     auto gpfifo_id = it->second;
-    auto& fifo = gpfifos[gpfifo_id];
+    auto& fifo = impl->gpfifos[gpfifo_id];
+    // Add the new new work to the channel.
     {
         std::scoped_lock lk2(fifo.guard);
         fifo.pending_work.emplace_back(std::move(entries));
     }
+
+    // If the current running FIFO is null or the one being pushed to then
+    // just return
+    if (impl->current_fifo == nullptr || impl->current_fifo == &fifo) {
+        return;
+    }
+
+    // If the current fifo has higher or equal priority to the current fifo then return
+    if (impl->current_fifo->info->priority >= fifo.info->priority) {
+        return;
+    }
+    // Mark scheduler update as required.
+    impl->must_reschedule = true;
 }
 
 void Scheduler::ChannelLoop(size_t gpfifo_id, s32 channel_id) {
+    auto& fifo = impl->gpfifos[gpfifo_id];
+    auto* channel_state = fifo.info.get();
+    const auto SendToPuller = [&] {
+        std::scoped_lock lk(fifo.guard);
+        if (fifo.pending_work.empty()) {
+            // Stop if no work available.
+            fifo.is_running = false;
+            return false;
+        }
+        // Otherwise, send work to puller and mark as running.
+        CommandList&& entries = std::move(fifo.pending_work.front());
+        channel_state->dma_pusher->Push(std::move(entries));
+        fifo.pending_work.pop_front();
+        fifo.is_running = true;
+        // Succeed.
+        return true;
+    };
+    // Inform the GPU about the current channel.
     gpu.BindChannel(channel_id);
-    auto& fifo = gpfifos[gpfifo_id];
     while (true) {
-        auto* channel_state = channels[channel_id].get();
-        fifo.guard.lock();
-        while (!fifo.pending_work.empty()) {
-            {
-
-                fifo.working.store(true, std::memory_order_release);
-                CommandList&& entries = std::move(fifo.pending_work.front());
-                channel_state->dma_pusher->Push(std::move(entries));
-                fifo.pending_work.pop_front();
-            }
-            fifo.guard.unlock();
+        while (SendToPuller()) {
+            // Execute.
             channel_state->dma_pusher->DispatchCalls();
-            fifo.guard.lock();
+            // Reschedule.
+            CheckStatus();
         }
-        fifo.working.store(false, std::memory_order_relaxed);
-        fifo.guard.unlock();
-        Common::Fiber::YieldTo(fifo.context, *master_control);
+        // Return to host execution when all work is completed.
+        Common::Fiber::YieldTo(fifo.context, *impl->master_control);
+        // Inform the GPU about the current channel.
         gpu.BindChannel(channel_id);
     }
 }
 
 void Scheduler::DeclareChannel(std::shared_ptr<ChannelState> new_channel) {
     s32 channel = new_channel->bind_id;
-    std::unique_lock lk(scheduling_guard);
-    channels.emplace(channel, new_channel);
+    std::unique_lock lk(impl->scheduling_guard);
+
     size_t new_fifo_id;
-    if (!free_fifos.empty()) {
-        new_fifo_id = free_fifos.front();
-        free_fifos.pop_front();
+    if (!impl->free_fifos.empty()) {
+        new_fifo_id = impl->free_fifos.front();
+        impl->free_fifos.pop_front();
     } else {
-        new_fifo_id = gpfifos.size();
-        gpfifos.emplace_back();
+        new_fifo_id = impl->gpfifos.size();
+        impl->gpfifos.emplace_back();
     }
-    auto& new_fifo = gpfifos[new_fifo_id];
-    channel_gpfifo_ids[channel] = new_fifo_id;
+    auto& new_fifo = impl->gpfifos[new_fifo_id];
+    impl->channel_gpfifo_ids[channel] = new_fifo_id;
     new_fifo.is_active = true;
     new_fifo.bind_id = channel;
     new_fifo.pending_work.clear();
+    new_fifo.info = new_channel;
+    new_fifo.scheduled_count = 0;
+    new_fifo.yield_count = 0;
+    new_fifo.is_running = false;
+    impl->schedule_priority_queue.try_emplace(new_channel->priority);
+    impl->schedule_priority_queue[new_channel->priority].push_back(new_fifo_id);
     std::function<void()> callback = std::bind(&Scheduler::ChannelLoop, this, new_fifo_id, channel);
     new_fifo.context = std::make_shared<Common::Fiber>(std::move(callback));
 }
diff --git a/src/video_core/control/scheduler.h b/src/video_core/control/scheduler.h
index c6f374f66a29..e9af75d9ce17 100644
--- a/src/video_core/control/scheduler.h
+++ b/src/video_core/control/scheduler.h
@@ -3,13 +3,11 @@
 
 #pragma once
 
-#include <atomic>
-#include <deque>
+#include <list>
 #include <memory>
-#include <mutex>
-#include <unordered_map>
 
-#include "common/fiber.h"
+#include "common/common_types.h"
+#include "video_core/control/channel_state.h"
 #include "video_core/dma_pusher.h"
 
 namespace Tegra {
@@ -35,26 +33,18 @@ class Scheduler {
 
     void DeclareChannel(std::shared_ptr<ChannelState> new_channel);
 
+    void ChangePriority(s32 channel_id, u32 new_priority);
+
 private:
     void ChannelLoop(size_t gpfifo_id, s32 channel_id);
+    bool ScheduleLevel(std::list<size_t>& queue);
+    void CheckStatus();
+    bool UpdateHighestPriorityChannel();
+
+    struct SchedulerImpl;
+    std::unique_ptr<SchedulerImpl> impl;
 
-    std::unordered_map<s32, std::shared_ptr<ChannelState>> channels;
-    std::unordered_map<s32, size_t> channel_gpfifo_ids;
-    std::mutex scheduling_guard;
-    std::shared_ptr<Common::Fiber> master_control;
-    struct GPFifoContext {
-        bool is_active;
-        std::shared_ptr<Common::Fiber> context;
-        std::deque<CommandList> pending_work;
-        std::atomic<bool> working{};
-        std::mutex guard;
-        s32 bind_id;
-    };
-    std::deque<GPFifoContext> gpfifos;
-    std::deque<size_t> free_fifos;
     GPU& gpu;
-    size_t current_fifo_rotation_id{};
-    GPFifoContext* current_fifo{};
 };
 
 } // namespace Control
